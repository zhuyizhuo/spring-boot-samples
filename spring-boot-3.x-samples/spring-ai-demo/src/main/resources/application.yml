server:
  port: 8083
  servlet:
    context-path: /

spring:
  profiles:
    active: siliconflow
    group:
      siliconflow: siliconflow
      deepseek: deepseek
      qwen: qwen
      glm: glm
      minimax: minimax
      ollama: ollama
  # Thymeleaf配置
  thymeleaf:
    cache: false
    prefix: classpath:/templates/
    suffix: .html
    mode: HTML
    encoding: UTF-8
  # Spring AI基础配置
  ai:
    openai:
      api-key: ${SILICONFLOW_API_KEY:dummy-key-for-profile-testing}
      base-url: https://api.siliconflow.cn/v1
      chat:
        options:
          model: deepseek-chat

# 应用程序配置
spring-ai-demo:
  # 会话管理配置
  session:
    max-history-length: 10  # 每个会话保留的最大历史消息数
    timeout-minutes: 60  # 会话超时时间（分钟）
  
  # 速率限制配置
  rate-limit:
    requests-per-minute: 60  # 每分钟最大请求数
    enabled: true  # 是否启用速率限制

# 日志配置
logging:
  level:
    root: INFO
    com.github.zhuyizhuo.ai.demo: DEBUG
    org.springframework.ai: DEBUG
    org.springframework.web.client: DEBUG
    org.springframework.ai.openai: DEBUG
    org.springframework.web.client.RestClient: DEBUG
    org.apache.http: DEBUG
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"

---
# 硅基流动配置 - 推荐配置
spring:
  config:
    activate:
      on-profile: siliconflow
  ai:
    openai:
      api-key: YOUR_SILICONFLOW_API_KEY_HERE
      base-url: https://api.siliconflow.cn
      chat:
        options:
          model: Qwen/Qwen2.5-7B-Instruct
          temperature: 0.7
          max-tokens: 2048

---
# DeepSeek官方配置
spring:
  config:
    activate:
      on-profile: deepseek
  ai:
    openai:
      api-key: YOUR_DEEPSEEK_API_KEY_HERE
      base-url: https://api.deepseek.com/v1
      chat:
        options:
          model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
          temperature: 0.3
          max-tokens: 4096

---
# 通义千问配置
spring:
  config:
    activate:
      on-profile: qwen
  ai:
    openai:
      api-key: YOUR_QWEN_API_KEY_HERE
      base-url: https://dashscope.aliyuncs.com/compatible-mode/v1
      chat:
        options:
          model: qwen-turbo
          temperature: 0.7
          max-tokens: 2000

---
# GLM智谱配置
spring:
  config:
    activate:
      on-profile: glm
  ai:
    openai:
      api-key: YOUR_GLM_API_KEY_HERE
      base-url: https://open.bigmodel.cn/api/paas/v4
      chat:
        options:
          model: chatglm-turbo
          temperature: 0.7
          max-tokens: 2048

---
# MiniMax配置
spring:
  config:
    activate:
      on-profile: minimax
  ai:
    openai:
      api-key: YOUR_MINIMAX_API_KEY_HERE
      base-url: https://api.minimax.chat/v1
      chat:
        options:
          model: minimax-latest
          temperature: 0.7
          max-tokens: 2048

---
# Ollama本地配置
spring:
  config:
    activate:
      on-profile: ollama
  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        options:
          model: llama2
          temperature: 0.7
          max-tokens: 2048